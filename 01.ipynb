{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1FATNRX-V4Yse0dYTrAhyxTOrJN2D6DIY","authorship_tag":"ABX9TyOFoUiZ1QIXPVpObTNKX6Ym"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-cNOyX03t8DM"},"outputs":[],"source":["from glob import glob\n","from tensorflow import keras\n","from IPython.display import Audio\n","from keras.models import Sequential\n","from keras.layers import Dense,LSTM,Dropout\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pandas as pd \n","import librosa as lr\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import os\n","import warnings\n","import librosa.display\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["data_directory = r'/content/drive/MyDrive/Colab Notebooks/Projects/Speech Emotion Recognition/Dataset/Emotions'\n","audio_files = glob(data_directory + '/*.wav')"],"metadata":{"id":"K_peSFD2wJiJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(audio_files)"],"metadata":{"id":"UkuOCxpEbTHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["paths = []\n","labels = []"],"metadata":{"id":"xXvOe86owKsY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for dirname,_,filenames in os.walk(r'/content/drive/MyDrive/Colab Notebooks/Projects/Speech Emotion Recognition/Dataset/Emotions'):\n","  for filename in filenames:\n","    paths.append(os.path.join(dirname,filename))\n","    print(filename)\n","    label = filename.split('_')[-1]\n","    print(label)\n","    label = label.split('.')[0]\n","    print(label.lower())\n","    labels.append(label.lower())"],"metadata":{"id":"diITQzfKBl0C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(paths)"],"metadata":{"id":"uDhDrDTiwNd_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["paths[:7]"],"metadata":{"id":"d8Zody1GwRos"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels[:7]"],"metadata":{"id":"_M2nPyvQwTAR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame()"],"metadata":{"id":"TmmaapGmwTem"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['speech'] = paths\n","df['label'] = labels"],"metadata":{"id":"ju80rU7kDQXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"G8ZgA-pdCkEg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['label'].value_counts()"],"metadata":{"id":"BF7oBwSowUnv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.countplot(df['label'])"],"metadata":{"id":"neLG7tDzwWVW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def waveplot(data,sr,emotion):\n","  plt.figure(figsize = (10,4))\n","  plt.title(emotion,size = 20)\n","  librosa.display.waveshow(data,sr = sr)\n","  plt.show()"],"metadata":{"id":"3Yd3So58wbxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def spectogram(data,sr,emotion):\n","  x = librosa.stft(data)\n","  xdb = librosa.amplitude_to_db(abs(x))\n","  plt.figure(figsize = (11,4))\n","  plt.title(emotion,size = 20)\n","  librosa.display.specshow(xdb,sr = sr,x_axis = 'time',y_axis = 'hz')\n","  plt.colorbar()"],"metadata":{"id":"Mf0Pg_5YCz-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emotion = 'angry'\n","path = np.array(df['speech'][df['label'] == emotion])[0]\n","data,sampling_rate = librosa.load(path)\n","waveplot(data,sampling_rate,emotion)\n","spectogram(data,sampling_rate,emotion)\n","Audio(path)"],"metadata":{"id":"yM010ROpweK2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emotion = 'disgust'\n","path = np.array(df['speech'][df['label'] == emotion])[0]\n","data,sampling_rate = librosa.load(path)\n","waveplot(data,sampling_rate,emotion)\n","spectogram(data,sampling_rate,emotion)\n","Audio(path)"],"metadata":{"id":"m6MFl2u9wfzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emotion = 'fear'\n","path = np.array(df['speech'][df['label'] == emotion])[0]\n","data,sampling_rate = librosa.load(path)\n","waveplot(data,sampling_rate,emotion)\n","spectogram(data,sampling_rate,emotion)\n","Audio(path)"],"metadata":{"id":"46anKqtSE4IH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emotion = 'happy'\n","path = np.array(df['speech'][df['label'] == emotion])[0]\n","data,sampling_rate = librosa.load(path)\n","waveplot(data,sampling_rate,emotion)\n","spectogram(data,sampling_rate,emotion)\n","Audio(path)"],"metadata":{"id":"TuDXzELBE6ae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emotion = 'neutral'\n","path = np.array(df['speech'][df['label'] == emotion])[0]\n","data,sampling_rate = librosa.load(path)\n","waveplot(data,sampling_rate,emotion)\n","spectogram(data,sampling_rate,emotion)\n","Audio(path)"],"metadata":{"id":"i6bitg0Owg4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emotion = 'ps'\n","path = np.array(df['speech'][df['label'] == emotion])[0]\n","data,sampling_rate = librosa.load(path)\n","waveplot(data,sampling_rate,emotion)\n","spectogram(data,sampling_rate,emotion)\n","Audio(path)"],"metadata":{"id":"fLufiedTE7_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emotion = 'sad'\n","path = np.array(df['speech'][df['label'] == emotion])[0]\n","data,sampling_rate = librosa.load(path)\n","waveplot(data,sampling_rate,emotion)\n","spectogram(data,sampling_rate,emotion)\n","Audio(path)"],"metadata":{"id":"a2fl2EWDwiF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_mfcc(filename):\n","  y,sr = librosa.load(filename,duration = 4,offset = 0.5)\n","  mfcc = np.mean(librosa.feature.mfcc(y = y,sr = sr,n_mfcc = 40).T,axis = 0)\n","  return mfcc"],"metadata":{"id":"tT7PM2Dcwl-F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extract_mfcc(df['speech'][0])"],"metadata":{"id":"jTivEtCKwns2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_mfcc = df['speech'].apply(lambda x: extract_mfcc(x))"],"metadata":{"id":"YSo4R3kMwpNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_mfcc"],"metadata":{"id":"-VoUCbu7wqRl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = [x for x in X_mfcc]\n","X = np.array(X)\n","X.shape"],"metadata":{"id":"YX1y7x3Jwqxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = np.expand_dims(X,-1)\n","X.shape"],"metadata":{"id":"foX8c6PpwsOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["enc = OneHotEncoder()\n","y = enc.fit_transform(df[['label']])"],"metadata":{"id":"M4ObdMzhwto9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y[0]"],"metadata":{"id":"_j3MXnmpwu1h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = y.toarray()"],"metadata":{"id":"_xz5aeCcww1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y"],"metadata":{"id":"uttHfOkSwx2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y.shape"],"metadata":{"id":"c5MArROBwzBf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train,x_test,y_train,y_test = train_test_split(X,y,random_state = 0,shuffle = True)"],"metadata":{"id":"eomp21rFw0U6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train.shape,y_train.shape,x_test.shape,y_test.shape"],"metadata":{"id":"Zxs7TQKBw46O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_1 = Sequential([\n","    LSTM(123,return_sequences = False,input_shape = (40,1)),\n","    Dense(64,activation = 'relu'),\n","    Dropout(0.2),\n","    Dense(32,activation = 'relu'),\n","    Dropout(0.2),\n","    Dense(7,activation = 'softmax')])"],"metadata":{"id":"ORK-DAM1w7QZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_1.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])"],"metadata":{"id":"KTR4tuQ3H1_O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_1.summary()"],"metadata":{"id":"_4hiIvkHH0u0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history_1 = model_1.fit(x_train,y_train,validation_data = (x_test,y_test),epochs = 80,batch_size = 512,shuffle = True)"],"metadata":{"id":"qwyLqUtKw9Ve"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Accuracy of the model on test data :\",model_1.evaluate(x_test,y_test)[1]*100,\"%\")"],"metadata":{"id":"fvsUl_rjw_tB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Accuracy of the model on train data :\",model_1.evaluate(x_train,y_train)[1]*100,\"%\")"],"metadata":{"id":"3MHhlBthxBPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = list(range(80))\n","\n","acc = history_1.history['accuracy']\n","val_acc = history_1.history['val_accuracy']\n","plt.plot(epochs,acc,label = 'Train Accuracy')\n","plt.plot(epochs,val_acc,label = 'Val Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"0KrxrE9pNG6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss = history_1.history['loss']\n","val_loss = history_1.history['val_loss']\n","plt.plot(epochs,loss,label = 'Train Loss')\n","plt.plot(epochs,val_loss,label = 'Val Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"auycs3nvNLNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_2 = keras.Sequential([\n","    keras.layers.Flatten(input_shape = (40,1)),\n","    keras.layers.Dense(128,activation = 'relu'),\n","    keras.layers.Dense(128,activation = 'relu'),\n","    keras.layers.Dense(7,activation = 'softmax')])"],"metadata":{"id":"4mzzo9fgxGpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_2.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])"],"metadata":{"id":"dhmdAtK_HsJ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_2.summary()"],"metadata":{"id":"eFxhUzJOHzB8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history_2 = model_2.fit(x_train,y_train,validation_data = (x_test,y_test),epochs = 80,batch_size = 512,shuffle = True)"],"metadata":{"id":"1s-rKu1BxJcJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Accuracy of the model on test data :\",model_2.evaluate(x_test,y_test)[1]*100,\"%\")"],"metadata":{"id":"BE25JKMNxKwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Accuracy of the model on train data :\",model_2.evaluate(x_train,y_train)[1]*100,\"%\")"],"metadata":{"id":"LkMtMRZjxLmE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = list(range(80))\n","\n","acc = history_2.history['accuracy']\n","val_acc = history_2.history['val_accuracy']\n","plt.plot(epochs,acc,label = 'Train Accuracy')\n","plt.plot(epochs,val_acc,label = 'Val Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"gYLVml4txUE5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss = history_2.history['loss']\n","val_loss = history_2.history['val_loss']\n","plt.plot(epochs,loss,label = 'Train Loss')\n","plt.plot(epochs,val_loss,label = 'Val Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"hDFF0KwqxVGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def func_pred_1(pred):\n","  prediction = model_1.predict(pred.reshape(-1,40,1))\n","  return labels[np.argmax(prediction)] "],"metadata":{"id":"58NPTzBvIsYO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_test_data():\n","  test_directory = r'/content/drive/MyDrive/Colab Notebooks/Projects/Speech Emotion Recognition/Test'\n","  audio_file = glob(test_directory + '/*.wav')\n","  val_paths = []\n","  for dirname,_,filenames in os.walk(r'/content/drive/MyDrive/Colab Notebooks/Projects/Speech Emotion Recognition/Test'):\n","    for filename in filenames:\n","      val_paths.append(os.path.join(dirname,filename))\n","      print(filename)\n","  df1 = pd.DataFrame()\n","  df1['val_speech'] = val_paths\n","  return df1\n","\n","temp = get_test_data()\n","for i in range(len(temp)):\n","    pred = extract_mfcc(temp['val_speech'][i])\n","    print(func_pred_1(pred))"],"metadata":{"id":"6iIEGLu6xYI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def func_pred_2(pred):\n","  prediction = model_2.predict(pred.reshape(-1,40,1))\n","  return labels[np.argmax(prediction)] "],"metadata":{"id":"v3pLaHZnNW8X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_test_data():\n","  test_directory = r'/content/drive/MyDrive/Colab Notebooks/Projects/Speech Emotion Recognition/Test'\n","  audio_file = glob(test_directory + '/*.wav')\n","  val_paths = []\n","  for dirname,_,filenames in os.walk(r'/content/drive/MyDrive/Colab Notebooks/Projects/Speech Emotion Recognition/Test'):\n","    for filename in filenames:\n","      val_paths.append(os.path.join(dirname,filename))\n","      print(filename)\n","  df1 = pd.DataFrame()\n","  df1['val_speech'] = val_paths\n","  return df1\n","\n","temp = get_test_data()\n","for i in range(len(temp)):\n","    pred = extract_mfcc(temp['val_speech'][i])\n","    print(func_pred_2(pred))"],"metadata":{"id":"nPAt_fpYNaXX"},"execution_count":null,"outputs":[]}]}